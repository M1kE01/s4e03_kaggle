{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "x2A2Bo4rBcL7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# First approach"
      ],
      "metadata": {
        "id": "x2A2Bo4rBcL7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjnFaFq3Kbo5"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "\n",
        "# Target encoding/decoding\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, auc, roc_curve, log_loss\n",
        "\n",
        "# Models\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier, plot_importance\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Math and DataFrame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Warnings ignore\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "original = pd.read_csv('/content/steel_plates_faults_original_dataset.csv')\n",
        "train.head()"
      ],
      "metadata": {
        "id": "gL7LQqc2Xz1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "FyfQd8VWX8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train, original], axis=0).drop_duplicates()\n",
        "train.head()"
      ],
      "metadata": {
        "id": "BwMDLxsIXLjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "LcS0cjg7XOXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop('id', axis=1)\n",
        "test = test.drop('id', axis=1)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "VwyyqZwsXOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_variables = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps',  'Other_Faults']"
      ],
      "metadata": {
        "id": "3Rb4-mcSXP2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the sum of the axis is greater than 1 because in the same row more than 1 columns value is 1, we can sure that is not a multi class problem, it is multi label problem.\n",
        "no_faults = train[train[target_variables].sum(axis=1) == 0][target_variables]\n",
        "print(f'\\nNumber of No Faults: {no_faults.shape[0]}\\n')\n",
        "no_faults"
      ],
      "metadata": {
        "id": "y0lGGGKRXTgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_faults.shape[0] / train.shape[0] * 100"
      ],
      "metadata": {
        "id": "IQr2nFXhXVYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "more_than_one_faults = train[train[target_variables].sum(axis=1) > 1][target_variables]\n",
        "print(f'\\nNumber of More than one faults: {more_than_one_faults.shape[0]}\\n')\n",
        "more_than_one_faults"
      ],
      "metadata": {
        "id": "B7vUQIT-tTsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train[target_variables].sum(axis=1) <= 1]\n",
        "train"
      ],
      "metadata": {
        "id": "4eLfdm5NtWn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a numpy array from our given targets.\n",
        "targets_arr = train[target_variables].values.copy()\n",
        "targets_arr"
      ],
      "metadata": {
        "id": "hDQHk5VltYfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_faults_arr = 1 - targets_arr.sum(axis=1)[:, np.newaxis]\n",
        "print(f'\\nNumber of the No Faults: {no_faults_arr.sum()}\\n')\n",
        "no_faults_arr"
      ],
      "metadata": {
        "id": "ahqIAILWtZd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_concatenated = np.concatenate([targets_arr, no_faults_arr], axis=1)\n",
        "targets_concatenated"
      ],
      "metadata": {
        "id": "SXfbgPDLtmaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(target_variables, axis=1)\n",
        "target = train[target_variables]\n",
        "X.head()"
      ],
      "metadata": {
        "id": "pE3Ukwdpto3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_sc = scaler.fit_transform(X)\n",
        "test_sc = scaler.transform(test)\n",
        "X_sc"
      ],
      "metadata": {
        "id": "e8csOSIYtqF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot encoded to label-encoded"
      ],
      "metadata": {
        "id": "vN5iqiPqfEDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoded_targets = np.argmax(targets_concatenated, axis=-1)\n",
        "label_encoded_targets"
      ],
      "metadata": {
        "id": "oCzaBI4ntsXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = np.unique(label_encoded_targets)\n",
        "unique_classes"
      ],
      "metadata": {
        "id": "-7HLhS2PfDh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=unique_classes, y=label_encoded_targets)\n",
        "class_weights"
      ],
      "metadata": {
        "id": "kXq-CwLSnFkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_param = {key: value for key, value in zip(unique_classes, class_weights)}\n",
        "class_weights_param"
      ],
      "metadata": {
        "id": "uK5lr3IUnL6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna tuned parameters"
      ],
      "metadata": {
        "id": "uhOauZ3OnO1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TUNE = False"
      ],
      "metadata": {
        "id": "CVqMgZ7RnNM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial, X_train, y_train, X_test, y_test):\n",
        "    # Define parameters to be optimized for the LGBMClassifier\n",
        "    param = {\n",
        "        \"class_weight\": class_weights_param,\n",
        "        \"objective\": \"multiclass\",\n",
        "        \"metric\": \"multi_logloss\",\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"random_state\": 42,\n",
        "        \"num_class\": 7,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.03),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 600),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.005, 0.025),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.02, 0.06),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 14),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 0.9),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 50),\n",
        "    }\n",
        "\n",
        "    # Create an instance of LGBMClassifier with the suggested parameters\n",
        "    lgbm_classifier = LGBMClassifier(**param)\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    lgbm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the classifier on the test data\n",
        "    score = lgbm_classifier.score(X_test, y_test)\n",
        "\n",
        "    print(f'SCORE: {score}')\n",
        "\n",
        "    return score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, label_encoded_targets, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n",
        "\n",
        "# Set up the sampler for Optuna optimization\n",
        "sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n",
        "\n",
        "# Create a study object for Optuna optimization\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "\n",
        "# If TUNE\n",
        "if TUNE:\n",
        "    # Run the optimization process\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=150)\n",
        "\n",
        "    # Get the best parameters after optimization\n",
        "    best_params = study.best_params"
      ],
      "metadata": {
        "id": "Xb4L6vsvnblb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial, X_train, y_train, X_test, y_test):\n",
        "    # Define parameters to be optimized for the XGBClassifier\n",
        "    param = {\n",
        "        \"objective\": 'multi:softmax',\n",
        "        \"booster\": 'gbtree',\n",
        "        \"random_state\": 42,\n",
        "        \"num_class\": 7,\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
        "        'gamma' : trial.suggest_float('gamma', 1e-9, 1.0),\n",
        "        'subsample': trial.suggest_float('subsample', 0.25, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.25, 1.0),\n",
        "        'max_depth': trial.suggest_int('max_depth', 0, 24),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 30),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),\n",
        "    }\n",
        "\n",
        "    # Create an instance of LGBMClassifier with the suggested parameters\n",
        "    xgb_classifier = XGBClassifier(**param)\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the classifier on the test data\n",
        "    score = xgb_classifier.score(X_test, y_test)\n",
        "    y_pred_prob = xgb_classifier.predict_proba(X_test)\n",
        "    lgbm_log_loss = log_loss(y_test, y_pred_prob)\n",
        "    print(f'SCORE: {score}')\n",
        "    print(f'Log Loss: {lgbm_log_loss}')\n",
        "\n",
        "    return score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, label_encoded_targets, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n",
        "\n",
        "# Set up the sampler for Optuna optimization\n",
        "sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n",
        "\n",
        "# Create a study object for Optuna optimization\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "\n",
        "if TUNE:\n",
        "\n",
        "    # Run the optimization process\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=150)\n",
        "\n",
        "    # Get the best parameters after optimization\n",
        "    best_params = study.best_params\n"
      ],
      "metadata": {
        "id": "lhuo5uIvnfDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_params: dict = {\n",
        "    \"class_weight\": class_weights_param, # Balanced class weight\n",
        "    \"objective\": \"multiclass\",          # Objective function for the model\n",
        "    \"metric\": \"multi_logloss\",          # Evaluation metric\n",
        "    \"verbosity\": -1,                    # Verbosity level (-1 for silent)\n",
        "    \"boosting_type\": \"gbdt\",            # Gradient boosting type\n",
        "    \"random_state\": 42,       # Random state for reproducibility\n",
        "    \"num_class\": 7,                     # Number of classes in the dataset\n",
        "    'n_estimators': 752,\n",
        "    'learning_rate': 0.005613916463106189,\n",
        "    'max_depth': 6,\n",
        "    'num_leaves': 252,\n",
        "    'subsample': 0.6045538705062335,\n",
        "    'colsample_bytree': 0.866501494211133,\n",
        "    'colsample_bynode': 0.5443098861233086,\n",
        "    'reg_alpha': 0.0024787490924597882,\n",
        "    'reg_lambda': 3.5334079815178954,\n",
        "    'min_split_gain': 0.05539710161546875,\n",
        "}"
      ],
      "metadata": {
        "id": "UxGVcZUpAEWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_params: dict = {\n",
        "    'class_weight': class_weights_param,\n",
        "    'objective':'multi:softmax',\n",
        "    'n_estimators': 829,\n",
        "    'learning_rate': 0.010260565670497695,\n",
        "    'gamma': 0.16282691057583543,\n",
        "    'reg_alpha': 0.010492176264956674,\n",
        "    'reg_lambda': 0.437536781187624,\n",
        "    'max_depth': 5,\n",
        "    'min_child_weight': 2,\n",
        "    'subsample': 0.6971737476610285,\n",
        "    'colsample_bytree': 0.5115061295805807,\n",
        "    'random_state': 345,\n",
        "}"
      ],
      "metadata": {
        "id": "AufUaJuJAHZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params: dict = {\n",
        "    'class_weights': class_weights_param,\n",
        "    'learning_rate': 0.13762007048684638,\n",
        "    'depth': 5,\n",
        "    'l2_leaf_reg': 5.285199432056192,\n",
        "    'bagging_temperature': 0.6029582154263095,\n",
        "    'random_seed': 42,\n",
        "    'verbose': False,\n",
        "    'iterations':1000,\n",
        "}"
      ],
      "metadata": {
        "id": "aRRSk6ghFw_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = [\n",
        "    ('XGB', XGBClassifier(**xgb_params)),\n",
        "    ('LGBM', LGBMClassifier(**lgbm_params)),\n",
        "    ('CAT', CatBoostClassifier(**cat_params))\n",
        "]"
      ],
      "metadata": {
        "id": "IySFUaXPFxNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHT_TUNE = False"
      ],
      "metadata": {
        "id": "rerzwzFpFzc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial, X_train, y_train, X_test, y_test):\n",
        "    # Define parameters to be optimized for the weighted ensemble\n",
        "\n",
        "    obj_estimators = [\n",
        "        ('XGB', XGBClassifier(**xgb_params)),\n",
        "        ('LGBM', LGBMClassifier(**lgbm_params)),\n",
        "        ('CAT', CatBoostClassifier(**cat_params))\n",
        "    ]\n",
        "\n",
        "    voting_classifier = VotingClassifier(\n",
        "        estimators=obj_estimators,\n",
        "        voting='soft',\n",
        "        weights=[\n",
        "            trial.suggest_float('XGB_Weight', 1.00, 9.00),\n",
        "            trial.suggest_float('LGBM_Weight', 0.25, 5.00),\n",
        "            trial.suggest_float('CAT_Weight', 0.25, 2.00),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "    predict_probs = voting_classifier.predict_proba(X_test)\n",
        "\n",
        "    auc_score = roc_auc_score(y_test, predict_probs, multi_class='ovr')\n",
        "\n",
        "    return auc_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, label_encoded_targets, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n",
        "\n",
        "# Set up the sampler for Optuna optimization\n",
        "weight_sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n",
        "\n",
        "# Create a study object for Optuna optimization\n",
        "weight_study = optuna.create_study(direction=\"maximize\", sampler=weight_sampler)\n",
        "\n",
        "if WEIGHT_TUNE:\n",
        "\n",
        "    # Run the optimization process\n",
        "    weight_study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=300)\n",
        "\n",
        "    # Get the best parameters after optimization\n",
        "    weight_best_params = weight_study.best_params"
      ],
      "metadata": {
        "id": "3AfFT-4S9nZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting_estimators = [\n",
        "    ('XGB', XGBClassifier(**xgb_params)),\n",
        "    ('LGBM', LGBMClassifier(**lgbm_params)),\n",
        "    ('CAT', CatBoostClassifier(**cat_params))\n",
        "]"
      ],
      "metadata": {
        "id": "4yFdbqjz9nbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting_classifier = VotingClassifier(\n",
        "    estimators=voting_estimators,\n",
        "    voting='soft',\n",
        "    weights=[\n",
        "            7.994761950304625,\n",
        "            0.46883567511191715,\n",
        "            0.4412142916220983\n",
        "        ],\n",
        "    )\n",
        "\n",
        "voting_classifier.fit(X_sc, label_encoded_targets)\n",
        "\n",
        "predicted_probs = voting_classifier.predict_proba(test_sc)[:, :-1] # Filtering out 8th's target (No Faults)"
      ],
      "metadata": {
        "id": "ahFWZZgIwA87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probs.shape"
      ],
      "metadata": {
        "id": "s_fctfT9wCa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.read_csv('/kaggle/input/playground-series-s4e3/sample_submission.csv')\n",
        "submission_df\n",
        "\n",
        "submission_df[target_variables] = predicted_probs\n",
        "submission_df"
      ],
      "metadata": {
        "id": "21_IEhTVwHZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "HAMOQLv2wKan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second approach"
      ],
      "metadata": {
        "id": "JeAfNwJFBgGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from prettytable import PrettyTable\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', font_scale=1.4)\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "tqdm_notebook.get_lock().locks = []\n",
        "# !pip install sweetviz\n",
        "# import sweetviz as sv\n",
        "import concurrent.futures\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from itertools import combinations\n",
        "import random\n",
        "from random import randint, uniform\n",
        "import gc\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from itertools import combinations\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xg\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\n",
        "from sklearn.cluster import KMeans\n",
        "!pip install yellowbrick\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "!pip install gap-stat\n",
        "from gap_statistic.optimalK import OptimalK\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import boxcox\n",
        "import math\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "!pip install optuna\n",
        "import optuna\n",
        "!pip install cmaes\n",
        "import cmaes\n",
        "import xgboost as xgb\n",
        "!pip install catboost\n",
        "!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n",
        "import lightgbm as lgb\n",
        "!pip install category_encoders\n",
        "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\n",
        "!pip install -U imbalanced-learn\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn.svm import NuSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from catboost import Pool\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.pandas.set_option('display.max_columns',None)"
      ],
      "metadata": {
        "id": "LOwQFGc4BhY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('/kaggle/input/playground-series-s4e3/train.csv')\n",
        "test=pd.read_csv('/kaggle/input/playground-series-s4e3/test.csv')\n",
        "original=pd.read_csv(\"/kaggle/input/steel-plates-faults/SteelPlatesFaults.csv\")\n",
        "\n",
        "train.drop(columns=[\"id\"],inplace=True)\n",
        "test.drop(columns=[\"id\"],inplace=True)\n",
        "\n",
        "train_copy=train.copy()\n",
        "test_copy=test.copy()\n",
        "original_copy=original.copy()\n",
        "\n",
        "print(original.shape)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "train=pd.concat([train,original],axis=0)\n",
        "train.reset_index(inplace=True,drop=True)\n",
        "\n",
        "target=['Pastry','Z_Scratch','K_Scatch','Stains','Dirtiness','Bumps','Other_Faults']\n",
        "\n",
        "original.head()"
      ],
      "metadata": {
        "id": "Ss5VdEizfUlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory analysis"
      ],
      "metadata": {
        "id": "n5pcH92Ue9YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cont_cols = test.columns\n",
        "colors = ['blue', 'orange', 'green']\n",
        "\n",
        "num_plots = len(cont_cols)\n",
        "num_cols = 3\n",
        "num_rows = -(-num_plots // num_cols)\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(21, 5 * num_rows))  # Adjust the figure size as needed\n",
        "\n",
        "for i, feature in enumerate(cont_cols):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "\n",
        "    ax = axes[row, col] if num_rows > 1 else axes[col]\n",
        "\n",
        "    sns.histplot(train_copy[feature], kde=True, color=colors[0], label='Train', alpha=0.5, bins=30, ax=ax)\n",
        "    sns.histplot(test_copy[feature], kde=True, color=colors[1], label='Test', alpha=0.5, bins=30, ax=ax)\n",
        "    sns.histplot(original[feature], kde=True, color=colors[2], label='Original', alpha=0.5, bins=30, ax=ax)\n",
        "\n",
        "    ax.set_title(f'Distribution of {feature}')\n",
        "    ax.set_xlabel(feature)\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.legend()\n",
        "\n",
        "if num_plots % num_cols != 0:\n",
        "    for j in range(num_plots % num_cols, num_cols):\n",
        "        axes[-1, j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r_TLs338e5E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering"
      ],
      "metadata": {
        "id": "Hcj5HYTifbVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OHE(train_df,test_df,cols,target):\n",
        "    '''\n",
        "    Function for one hot encoding, it first combined the data so that no category is missed and\n",
        "    the category with least frequency can be dropped because of redunancy\n",
        "    '''\n",
        "    combined = pd.concat([train_df, test_df], axis=0)\n",
        "    for col in cols:\n",
        "        one_hot = pd.get_dummies(combined[col]).astype(int)\n",
        "        counts = combined[col].value_counts()\n",
        "        min_count_category = counts.idxmin()\n",
        "        one_hot = one_hot.drop(min_count_category, axis=1)\n",
        "        one_hot.columns=[str(f)+col for f in one_hot.columns]\n",
        "        combined = pd.concat([combined, one_hot], axis=\"columns\")\n",
        "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
        "\n",
        "    # split back to train and test dataframes\n",
        "    train_ohe = combined[:len(train_df)]\n",
        "    test_ohe = combined[len(train_df):]\n",
        "    test_ohe.reset_index(inplace=True,drop=True)\n",
        "    test_ohe.drop(columns=[target],inplace=True)\n",
        "    return train_ohe, test_ohe"
      ],
      "metadata": {
        "id": "wik1uLvQfC7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [f for f in test.columns if test[f].nunique()/test.shape[0]*100<5 and test[f].nunique()>2 ]\n",
        "test[cat_cols].nunique()\n",
        "\n",
        "def nearest_val(target):\n",
        "    return min(common, key=lambda x: abs(x - target))\n",
        "\n",
        "global cat_cols_updated\n",
        "cat_cols_updated=[]\n",
        "for col in cat_cols:\n",
        "    train[f\"{col}_cat\"]=train[col]\n",
        "    test[f\"{col}_cat\"]=test[col]\n",
        "    cat_cols_updated.append(f\"{col}_cat\")\n",
        "    uncommon=list((set(test[col].unique())| set(train[col].unique()))-(set(test[col].unique())& set(train[col].unique())))\n",
        "    if uncommon:\n",
        "        common=list(set(test[col].unique())& set(train[col].unique()))\n",
        "        train[f\"{col}_cat\"]=train[col].apply(nearest_val)\n",
        "        test[f\"{col}_cat\"]=test[col].apply(nearest_val)"
      ],
      "metadata": {
        "id": "HVvzGWQCfm8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def high_freq_ohe(train, test, extra_cols, target, n_limit=50):\n",
        "    '''\n",
        "    If you wish to apply one hot encoding on a feature with so many unique values, then this can be applied,\n",
        "    where it takes a maximum of n categories and drops the rest of them treating as rare categories\n",
        "    '''\n",
        "    train_copy=train.copy()\n",
        "    test_copy=test.copy()\n",
        "    ohe_cols=[]\n",
        "    for col in extra_cols:\n",
        "        dict1=train_copy[col].value_counts().to_dict()\n",
        "        ordered=dict(sorted(dict1.items(), key=lambda x: x[1], reverse=True))\n",
        "        rare_keys=list([*ordered.keys()][n_limit:])\n",
        "#         ext_keys=[f[0] for f in ordered.items() if f[1]<50]\n",
        "        rare_key_map=dict(zip(rare_keys, np.full(len(rare_keys),9999)))\n",
        "\n",
        "        train_copy[col]=train_copy[col].replace(rare_key_map)\n",
        "        test_copy[col]=test_copy[col].replace(rare_key_map)\n",
        "    train_copy, test_copy = OHE(train_copy, test_copy, extra_cols, target)\n",
        "    drop_cols=[f for f in train_copy.columns if \"9999\" in f or train_copy[f].nunique()==1]\n",
        "    train_copy=train_copy.drop(columns=drop_cols)\n",
        "    test_copy=test_copy.drop(columns=drop_cols)\n",
        "\n",
        "    return train_copy, test_copy\n",
        "\n",
        "def cat_encoding(train, test, target):\n",
        "    global overall_best_score\n",
        "    global overall_best_col\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Feature', 'Encoded Features', 'Log Loss Score']\n",
        "    train_copy=train.copy()\n",
        "    test_copy=test.copy()\n",
        "    train_dum = train.copy()\n",
        "    for feature in cat_cols_updated:\n",
        "#         print(feature)\n",
        "#         cat_labels = train_dum.groupby([feature])[target].mean().sort_values().index\n",
        "#         cat_labels2 = {k: i for i, k in enumerate(cat_labels, 0)}\n",
        "#         train_copy[feature + \"_target\"] = train[feature].map(cat_labels2)\n",
        "#         test_copy[feature + \"_target\"] = test[feature].map(cat_labels2)\n",
        "\n",
        "        dic = train[feature].value_counts().to_dict()\n",
        "        train_copy[feature + \"_count\"] =train[feature].map(dic)\n",
        "        test_copy[feature + \"_count\"] = test[feature].map(dic)\n",
        "\n",
        "        dic2=train[feature].value_counts().to_dict()\n",
        "#         list1=np.arange(len(dic2.values()),0,-1) # Higher rank for high count\n",
        "        list1=np.arange(len(dic2.values())) # Higher rank for low count\n",
        "        dic3=dict(zip(list(dic2.keys()),list1))\n",
        "\n",
        "        train_copy[feature+\"_count_label\"]=train[feature].replace(dic3).astype(float)\n",
        "        test_copy[feature+\"_count_label\"]=test[feature].replace(dic3).astype(float)\n",
        "\n",
        "        temp_cols = [ feature + \"_count\", feature + \"_count_label\"]#feature + \"_target\",\n",
        "\n",
        "\n",
        "\n",
        "        if train_copy[feature].nunique()<=5:\n",
        "            train_copy[feature]=train_copy[feature].astype(str)+\"_\"+feature\n",
        "            test_copy[feature]=test_copy[feature].astype(str)+\"_\"+feature\n",
        "            train_copy, test_copy = OHE(train_copy, test_copy, [feature], target)\n",
        "\n",
        "        else:\n",
        "            train_copy,test_copy=high_freq_ohe(train_copy,test_copy,[feature], target, n_limit=5)\n",
        "        train_copy=train_copy.drop(columns=[feature])\n",
        "        test_copy=test_copy.drop(columns=[feature])\n",
        "\n",
        "\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "        auc_scores = []\n",
        "\n",
        "        for f in temp_cols:\n",
        "            X = train_copy[[f]].values\n",
        "            y = train_copy[target].astype(int).values\n",
        "\n",
        "            auc = []\n",
        "            for train_idx, val_idx in kf.split(X, y):\n",
        "                X_train, y_train = X[train_idx], y[train_idx]\n",
        "                x_val, y_val = X[val_idx], y[val_idx]\n",
        "                model =  HistGradientBoostingClassifier (max_iter=300, learning_rate=0.02, max_depth=6, random_state=42)\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict_proba(x_val)[:,1]\n",
        "                auc.append(roc_auc_score(y_val,  y_pred))\n",
        "            auc_scores.append((f, np.mean(auc)))\n",
        "\n",
        "        best_col, best_auc = sorted(auc_scores, key=lambda x: x[1], reverse=True)[0]\n",
        "\n",
        "        corr = train_copy[temp_cols].corr(method='pearson')\n",
        "        corr_with_best_col = corr[best_col]\n",
        "        cols_to_drop = [f for f in temp_cols if corr_with_best_col[f] > 0.5 and f != best_col]\n",
        "        final_selection = [f for f in temp_cols if f not in cols_to_drop]\n",
        "        if cols_to_drop:\n",
        "            train_copy = train_copy.drop(columns=cols_to_drop)\n",
        "            test_copy = test_copy.drop(columns=cols_to_drop)\n",
        "\n",
        "        table.add_row([feature, best_col, best_auc])\n",
        "#         print(feature)\n",
        "\n",
        "#     print(table)\n",
        "    return train_copy, test_copy"
      ],
      "metadata": {
        "id": "D5o-EX_3fTZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Splitter:\n",
        "    def __init__(self, test_size=0.2, kfold=True, n_splits=5):\n",
        "        self.test_size = test_size\n",
        "        self.kfold = kfold\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split_data(self, X, y, random_state_list):\n",
        "        if self.kfold:\n",
        "            for random_state in random_state_list:\n",
        "                kf = KFold(n_splits=self.n_splits, random_state=random_state, shuffle=True)\n",
        "                for train_index, val_index in kf.split(X, y):\n",
        "                    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "                    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
        "                    yield X_train, X_val, y_train, y_val\n",
        "\n",
        "class Classifier:\n",
        "    def __init__(self,n_estimators=100, device=\"cpu\", random_state=0):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.device = device\n",
        "        self.random_state = random_state\n",
        "        self.models = self._define_model()\n",
        "        self.len_models = len(self.models)\n",
        "\n",
        "    def _define_model(self):\n",
        "        xgb_params = {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_depth': 4,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.1,\n",
        "            'n_jobs': -1,\n",
        "            'eval_metric': 'logloss',\n",
        "            'objective': 'binary:logistic',\n",
        "            'tree_method': 'hist',\n",
        "            'verbosity': 0,\n",
        "            'random_state': self.random_state,\n",
        "#             'class_weight':class_weights_dict,\n",
        "        }\n",
        "        if self.device == 'gpu':\n",
        "            xgb_params['tree_method'] = 'gpu_hist'\n",
        "            xgb_params['predictor'] = 'gpu_predictor'\n",
        "\n",
        "        xgb_params2=xgb_params.copy()\n",
        "        xgb_params2['subsample']= 0.5\n",
        "        xgb_params2['max_depth']=9\n",
        "        xgb_params2['learning_rate']=0.045\n",
        "        xgb_params2['colsample_bytree']=0.3\n",
        "\n",
        "        xgb_params3=xgb_params.copy()\n",
        "        xgb_params3['subsample']= 0.6\n",
        "        xgb_params3['max_depth']=6\n",
        "        xgb_params3['learning_rate']=0.02\n",
        "        xgb_params3['colsample_bytree']=0.7\n",
        "\n",
        "        xgb_params4=xgb_params.copy()\n",
        "        xgb_params4['subsample']= 0.5943421542786502\n",
        "        xgb_params4['max_depth']=6\n",
        "        xgb_params4['learning_rate']=0.109\n",
        "        xgb_params4['colsample_bytree']=0.5595039093313848\n",
        "        lgb_params = {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_depth': 8,\n",
        "            'learning_rate': 0.02,\n",
        "            'subsample': 0.20,\n",
        "            'colsample_bytree': 0.56,\n",
        "            'reg_alpha': 0.25,\n",
        "            'reg_lambda': 5e-08,\n",
        "            'objective': 'binary',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'device': self.device,\n",
        "            'random_state': self.random_state,\n",
        "            'verbose':-1,\n",
        "#             'class_weight':class_weights_dict,\n",
        "        }\n",
        "        lgb_params2 = {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_depth': 5,\n",
        "            'learning_rate': 0.015,\n",
        "            'subsample': 0.50,\n",
        "            'colsample_bytree': 0.1,\n",
        "            'reg_alpha': 0.07608657669988828,\n",
        "            'reg_lambda': 0.2255036530113883,\n",
        "            'objective': 'binary',\n",
        "            'boosting_type': 'gbdt',\n",
        "            'device': self.device,\n",
        "            'random_state': self.random_state,\n",
        "        }\n",
        "        lgb_params3=lgb_params.copy()\n",
        "        lgb_params3['subsample']=0.9\n",
        "        lgb_params3['reg_lambda']=0.3461495211744402\n",
        "        lgb_params3['reg_alpha']=0.3095626288582237\n",
        "        lgb_params3['max_depth']=8\n",
        "        lgb_params3['learning_rate']=0.007\n",
        "        lgb_params3['colsample_bytree']=0.5\n",
        "\n",
        "        lgb_params4=lgb_params2.copy()\n",
        "        lgb_params4['subsample']=0.3\n",
        "        lgb_params4['reg_lambda']=0.49406951573373614\n",
        "        lgb_params4['reg_alpha']=0.16269100796945424\n",
        "        lgb_params4['max_depth']=9\n",
        "        lgb_params4['learning_rate']=0.117\n",
        "        lgb_params4['colsample_bytree']=0.3\n",
        "\n",
        "        cb_params = {\n",
        "            'iterations': self.n_estimators,\n",
        "            'depth': 13,\n",
        "            'learning_rate': 0.015,\n",
        "            'l2_leaf_reg': 0.5,\n",
        "            'random_strength': 0.1,\n",
        "            'max_bin': 200,\n",
        "            'od_wait': 65,\n",
        "            'one_hot_max_size': 50,\n",
        "            'grow_policy': 'Depthwise',\n",
        "            'bootstrap_type': 'Bernoulli',\n",
        "            'od_type': 'Iter',\n",
        "            'eval_metric': 'AUC',\n",
        "            'loss_function': 'Logloss',\n",
        "            'task_type': self.device.upper(),\n",
        "            'random_state': self.random_state,\n",
        "        }\n",
        "        cb_sym_params = cb_params.copy()\n",
        "        cb_sym_params['grow_policy'] = 'SymmetricTree'\n",
        "        cb_loss_params = cb_params.copy()\n",
        "        cb_loss_params['grow_policy'] = 'Lossguide'\n",
        "\n",
        "        cb_params2=  cb_params.copy()\n",
        "        cb_params2['learning_rate']=0.01\n",
        "        cb_params2['depth']=8\n",
        "\n",
        "        cb_params3={\n",
        "            'iterations': self.n_estimators,\n",
        "            'random_strength': 0.5783342241486167,\n",
        "            'one_hot_max_size': 10,\n",
        "            'max_bin': 150,\n",
        "            'learning_rate': 0.177,\n",
        "            'l2_leaf_reg': 0.705662073971363,\n",
        "            'grow_policy': 'SymmetricTree',\n",
        "            'depth': 5,\n",
        "            'max_bin': 200,\n",
        "            'od_wait': 65,\n",
        "            'bootstrap_type': 'Bayesian',\n",
        "            'od_type': 'Iter',\n",
        "            'eval_metric': 'AUC',\n",
        "            'loss_function': 'Logloss',\n",
        "            'task_type': self.device.upper(),\n",
        "            'random_state': self.random_state,\n",
        "        }\n",
        "        cb_params4=  cb_params.copy()\n",
        "        cb_params4['learning_rate']=0.01\n",
        "        cb_params4['depth']=12\n",
        "        dt_params= {'min_samples_split': 30, 'min_samples_leaf': 10, 'max_depth': 8, 'criterion': 'gini'}\n",
        "\n",
        "        models = {\n",
        "            'xgb': xgb.XGBClassifier(**xgb_params),\n",
        "#            'xgb2': xgb.XGBClassifier(**xgb_params2),\n",
        "#            'xgb3': xgb.XGBClassifier(**xgb_params3),\n",
        "#            'xgb4': xgb.XGBClassifier(**xgb_params4),\n",
        "#            'lgb': lgb.LGBMClassifier(**lgb_params),\n",
        "#             'lgb2': lgb.LGBMClassifier(**lgb_params2),\n",
        "#             'lgb3': lgb.LGBMClassifier(**lgb_params3),\n",
        "#             'lgb4': lgb.LGBMClassifier(**lgb_params4),\n",
        "            'cat': CatBoostClassifier(**cb_params),\n",
        "#            'cat2': CatBoostClassifier(**cb_params2),\n",
        "#             'cat3': CatBoostClassifier(**cb_params3),\n",
        "#             'cat4': CatBoostClassifier(**cb_params4),\n",
        "             \"cat_sym\": CatBoostClassifier(**cb_sym_params),\n",
        "#             \"cat_loss\": CatBoostClassifier(**cb_loss_params)\n",
        "\n",
        "        }\n",
        "        return models\n"
      ],
      "metadata": {
        "id": "FWps0pNUfaEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptunaWeights:\n",
        "    def __init__(self, random_state, n_trials=5000):\n",
        "        self.study = None\n",
        "        self.weights = None\n",
        "        self.random_state = random_state\n",
        "        self.n_trials = n_trials\n",
        "\n",
        "    def _objective(self, trial, y_true, y_preds):\n",
        "        # Define the weights for the predictions from each model\n",
        "        weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds))]\n",
        "\n",
        "        # Calculate the weighted prediction\n",
        "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n",
        "\n",
        "        auc_score = roc_auc_score(y_true, weighted_pred)\n",
        "        log_loss_score=log_loss(y_true, weighted_pred)\n",
        "        return auc_score#/log_loss_score\n",
        "\n",
        "    def fit(self, y_true, y_preds):\n",
        "        optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n",
        "        pruner = optuna.pruners.HyperbandPruner()\n",
        "        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='maximize')\n",
        "        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n",
        "        self.study.optimize(objective_partial, n_trials=self.n_trials)\n",
        "        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n",
        "\n",
        "    def predict(self, y_preds):\n",
        "        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n",
        "        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n",
        "        return weighted_pred\n",
        "\n",
        "    def fit_predict(self, y_true, y_preds):\n",
        "        self.fit(y_true, y_preds)\n",
        "        return self.predict(y_preds)\n",
        "\n",
        "    def weights(self):\n",
        "        return self.weights\n"
      ],
      "metadata": {
        "id": "KdE32sstexAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(X_train,X_test,y_train):\n",
        "    kfold = True\n",
        "    n_splits = 1 if not kfold else 5\n",
        "    random_state = 2023\n",
        "    random_state_list = [42] # used by split_data [71]\n",
        "    n_estimators = 9999 # 9999\n",
        "    early_stopping_rounds = 300\n",
        "    verbose = False\n",
        "\n",
        "    splitter = Splitter(kfold=kfold, n_splits=n_splits)\n",
        "\n",
        "    # Initialize an array for storing test predictions\n",
        "    test_predss = np.zeros(X_test.shape[0])\n",
        "    y_train_pred=y_train.copy()\n",
        "\n",
        "    ensemble_score = []\n",
        "    weights = []\n",
        "    trained_models = {'xgb':[], 'lgb':[]}\n",
        "\n",
        "\n",
        "    for i, (X_train_, X_val, y_train_, y_val) in enumerate(splitter.split_data(X_train, y_train, random_state_list=random_state_list)):\n",
        "        n = i % n_splits\n",
        "        m = i // n_splits\n",
        "\n",
        "        # Get a set of Regressor models\n",
        "        classifier = Classifier(n_estimators, device, random_state)\n",
        "        models = classifier.models\n",
        "\n",
        "        # Initialize lists to store oof and test predictions for each base model\n",
        "        oof_preds = []\n",
        "        test_preds = []\n",
        "\n",
        "        # Loop over each base model and fit it to the training data, evaluate on validation data, and store predictions\n",
        "        for name, model in models.items():\n",
        "            if ('cat' in name) or (\"lgb\" in name) or (\"xgb\" in name):\n",
        "                if 'lgb' in name: #categorical_feature=cat_features\n",
        "                    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)]),#,categorical_feature=cat_features,)\n",
        "                elif 'cat' in name:\n",
        "                    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)],#cat_features=cat_features,\n",
        "                              early_stopping_rounds=early_stopping_rounds, verbose=verbose)\n",
        "                else:\n",
        "                    model.fit(X_train_, y_train_, eval_set=[(X_val, y_val)], early_stopping_rounds=early_stopping_rounds, verbose=0)\n",
        "            else:\n",
        "                model.fit(X_train_, y_train_)\n",
        "\n",
        "            test_pred = model.predict_proba(X_test)[:, 1]\n",
        "            y_val_pred = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "            score = roc_auc_score(y_val, y_val_pred.reshape(-1, 1))\n",
        "    #         score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n",
        "\n",
        "            print(f'{name} [FOLD-{n} SEED-{random_state_list[m]}] ROC AUC score: {score:.5f}')\n",
        "\n",
        "            oof_preds.append(y_val_pred)\n",
        "            test_preds.append(test_pred)\n",
        "\n",
        "            if name in trained_models.keys():\n",
        "                trained_models[f'{name}'].append(deepcopy(model))\n",
        "        # Use Optuna to find the best ensemble weights\n",
        "        optweights = OptunaWeights(random_state=random_state)\n",
        "        y_val_pred = optweights.fit_predict(y_val.values, oof_preds)\n",
        "\n",
        "        score = roc_auc_score(y_val, y_val_pred.reshape(-1, 1))\n",
        "    #     score = accuracy_score(y_val, acc_cutoff_class(y_val, y_val_pred))\n",
        "        print(f'Ensemble [FOLD-{n} SEED-{random_state_list[m]}] ------------------>  ROC AUC score {score:.5f}')\n",
        "        ensemble_score.append(score)\n",
        "        weights.append(optweights.weights)\n",
        "\n",
        "        test_predss += optweights.predict(test_preds) / (n_splits * len(random_state_list))\n",
        "        y_train_pred.loc[y_val.index]=np.array(y_val_pred)\n",
        "\n",
        "        gc.collect()\n",
        "    # Calculate the mean ROC AUC  score of the ensemble\n",
        "    mean_score = np.mean(ensemble_score)\n",
        "    std_score = np.std(ensemble_score)\n",
        "    print(f'Ensemble ROC AUC score {mean_score:.5f} ± {std_score:.5f}')\n",
        "\n",
        "    # Print the mean and standard deviation of the ensemble weights for each model\n",
        "    print('--- Model Weights ---')\n",
        "    mean_weights = np.mean(weights, axis=0)\n",
        "    std_weights = np.std(weights, axis=0)\n",
        "    for name, mean_weight, std_weight in zip(models.keys(), mean_weights, std_weights):\n",
        "        print(f'{name}: {mean_weight:.5f} ± {std_weight:.5f}')\n",
        "    print(f'Overall OOF Preds AUC SCORE {roc_auc_score(y_train,y_train_pred)}')\n",
        "\n",
        "    print(\"__________________________________________________________________\")\n",
        "    return test_predss"
      ],
      "metadata": {
        "id": "6uSkLDMZrRtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_processor(train, test):\n",
        "    cols=test.columns.tolist()\n",
        "    train_cop=train.copy()\n",
        "    test_cop=test.copy()\n",
        "    drop_cols=[]\n",
        "    for i, feature in enumerate(cols):\n",
        "        for j in range(i+1, len(cols)):\n",
        "            if sum(abs(train_cop[feature]-train_cop[cols[j]]))==0:\n",
        "                if cols[j] not in drop_cols:\n",
        "                    drop_cols.append(cols[j])\n",
        "    print(drop_cols)\n",
        "    train_cop.drop(columns=drop_cols,inplace=True)\n",
        "    test_cop.drop(columns=drop_cols,inplace=True)\n",
        "\n",
        "    return train_cop, test_cop\n"
      ],
      "metadata": {
        "id": "qhvdNVNx2eRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission=pd.read_csv(\"/kaggle/input/playground-series-s4e3/sample_submission.csv\")\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "H9rJQPDl2kDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for col in target:\n",
        "    train_temp=train[test.columns.tolist()+[col]]\n",
        "    test_temp=test.copy()\n",
        "    train_temp, test_temp= cat_encoding(train_temp, test_temp, col)\n",
        "\n",
        "    final_features=test.columns.tolist()\n",
        "    sc=StandardScaler()\n",
        "\n",
        "    train_scaled=train_temp.copy()\n",
        "    test_scaled=test_temp.copy()\n",
        "\n",
        "    train_scaled[final_features]=sc.fit_transform(train[final_features])\n",
        "    test_scaled[final_features]=sc.transform(test[final_features])\n",
        "\n",
        "#     train_cop, test_cop=   post_processor(train_scaled, test_scaled)\n",
        "    train_cop, test_cop= train_scaled, test_scaled\n",
        "    X_train = train_cop.drop(columns=[col])\n",
        "    y_train = train_cop[col]\n",
        "\n",
        "    X_test = test_cop.copy()\n",
        "\n",
        "    test_predss=fit_model(X_train,X_test,y_train)\n",
        "    submission[col]=test_predss\n",
        "\n",
        "    count+=1\n",
        "    print(f'Column {col}, loop # {count}')"
      ],
      "metadata": {
        "id": "PX9A8D1c2nUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"submission_pure.csv\",index=False)\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "mT6DMWTh2qi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub1=pd.read_csv(\"/kaggle/input/multiclass-feature-engineering-thoughts/submission.csv\")\n",
        "sub2=pd.read_csv(\"/kaggle/input/ps4e03-multi-class-lightgbm/submission.csv\")\n",
        "\n",
        "sub_list=[sub1, sub2,submission]\n",
        "# weights=np.random.randint(1,4,len(sub_list))\n",
        "weights=[1,1,1]\n",
        "\n",
        "weighted_list = [item for sublist, weight in zip(sub_list, weights) for item in [sublist] * weight]\n",
        "\n",
        "def ensemble_mean(sub_list,cols, mean=\"AM\"):\n",
        "\n",
        "    sub_out=sub_list[0].copy()\n",
        "    if mean==\"AM\":\n",
        "        for col in cols:\n",
        "            sub_out[col]=sum(df[col] for df in sub_list)/len(sub_list)\n",
        "    elif mean==\"GM\":\n",
        "        for df in sub_list[1:]:\n",
        "\n",
        "            for col in cols:\n",
        "                sub_out[col]*=df[col]\n",
        "        for col in cols:\n",
        "            sub_out[col]=(sub_out[col])**(1/len(sub_list))\n",
        "    elif mean==\"HM\":\n",
        "        for col in cols:\n",
        "            sub_out[col]=len(sub_list)/sum(1/df[col] for df in sub_list)\n",
        "    sub_out[cols]=sub_out[cols].div(sub_out[cols].sum(axis=1), axis=0)\n",
        "\n",
        "    return sub_out\n",
        "\n",
        "sub_ensemble=ensemble_mean(weighted_list,target,mean=\"AM\")\n",
        "sub_ensemble.to_csv('submission.csv',index=False)\n",
        "sub_ensemble.head()"
      ],
      "metadata": {
        "id": "ZQLvCuG22umN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third approach"
      ],
      "metadata": {
        "id": "zRRA4ZiSJOHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from category_encoders import OneHotEncoder, CatBoostEncoder, MEstimateEncoder\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
        "\n",
        "from sklearn import set_config\n",
        "import os\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.base import clone\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from optuna.samplers import TPESampler\n",
        "import optuna\n",
        "pd.set_option(\"display.max_rows\",100)\n",
        "\n",
        "import tensorflow as tf\n",
        "import random\n",
        "os.environ['PYTHONHASHSEED'] = '51'\n",
        "random.seed(89)"
      ],
      "metadata": {
        "id": "zl790UlmJcaQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}