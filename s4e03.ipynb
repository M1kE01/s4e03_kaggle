{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjnFaFq3Kbo5"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "\n",
        "# Target encoding/decoding\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, auc, roc_curve, log_loss\n",
        "\n",
        "# Models\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier, plot_importance\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Math and DataFrame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Warnings ignore\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "original = pd.read_csv('/content/steel_plates_faults_original_dataset.csv')\n",
        "train.head()"
      ],
      "metadata": {
        "id": "gL7LQqc2Xz1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "FyfQd8VWX8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train, original], axis=0).drop_duplicates()\n",
        "train.head()"
      ],
      "metadata": {
        "id": "BwMDLxsIXLjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "LcS0cjg7XOXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop('id', axis=1)\n",
        "test = test.drop('id', axis=1)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "VwyyqZwsXOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_variables = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps',  'Other_Faults']"
      ],
      "metadata": {
        "id": "3Rb4-mcSXP2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the sum of the axis is greater than 1 because in the same row more than 1 columns value is 1, we can sure that is not a multi class problem, it is multi label problem.\n",
        "no_faults = train[train[target_variables].sum(axis=1) == 0][target_variables]\n",
        "print(f'\\nNumber of No Faults: {no_faults.shape[0]}\\n')\n",
        "no_faults"
      ],
      "metadata": {
        "id": "y0lGGGKRXTgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_faults.shape[0] / train.shape[0] * 100"
      ],
      "metadata": {
        "id": "IQr2nFXhXVYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "more_than_one_faults = train[train[target_variables].sum(axis=1) > 1][target_variables]\n",
        "print(f'\\nNumber of More than one faults: {more_than_one_faults.shape[0]}\\n')\n",
        "more_than_one_faults"
      ],
      "metadata": {
        "id": "B7vUQIT-tTsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train[target_variables].sum(axis=1) <= 1]\n",
        "train"
      ],
      "metadata": {
        "id": "4eLfdm5NtWn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a numpy array from our given targets.\n",
        "targets_arr = train[target_variables].values.copy()\n",
        "targets_arr"
      ],
      "metadata": {
        "id": "hDQHk5VltYfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_faults_arr = 1 - targets_arr.sum(axis=1)[:, np.newaxis]\n",
        "print(f'\\nNumber of the No Faults: {no_faults_arr.sum()}\\n')\n",
        "no_faults_arr"
      ],
      "metadata": {
        "id": "ahqIAILWtZd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_concatenated = np.concatenate([targets_arr, no_faults_arr], axis=1)\n",
        "targets_concatenated"
      ],
      "metadata": {
        "id": "SXfbgPDLtmaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(target_variables, axis=1)\n",
        "target = train[target_variables]\n",
        "X.head()"
      ],
      "metadata": {
        "id": "pE3Ukwdpto3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_sc = scaler.fit_transform(X)\n",
        "test_sc = scaler.transform(test)\n",
        "X_sc"
      ],
      "metadata": {
        "id": "e8csOSIYtqF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot encoded to label-encoded"
      ],
      "metadata": {
        "id": "vN5iqiPqfEDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoded_targets = np.argmax(targets_concatenated, axis=-1)\n",
        "label_encoded_targets"
      ],
      "metadata": {
        "id": "oCzaBI4ntsXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = np.unique(label_encoded_targets)\n",
        "unique_classes"
      ],
      "metadata": {
        "id": "-7HLhS2PfDh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=unique_classes, y=label_encoded_targets)\n",
        "class_weights"
      ],
      "metadata": {
        "id": "kXq-CwLSnFkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_param = {key: value for key, value in zip(unique_classes, class_weights)}\n",
        "class_weights_param"
      ],
      "metadata": {
        "id": "uK5lr3IUnL6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna tuned parameters"
      ],
      "metadata": {
        "id": "uhOauZ3OnO1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TUNE = False"
      ],
      "metadata": {
        "id": "CVqMgZ7RnNM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial, X_train, y_train, X_test, y_test):\n",
        "    # Define parameters to be optimized for the LGBMClassifier\n",
        "    param = {\n",
        "        \"class_weight\": class_weights_param,\n",
        "        \"objective\": \"multiclass\",\n",
        "        \"metric\": \"multi_logloss\",\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"random_state\": 42,\n",
        "        \"num_class\": 7,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.03),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 600),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.005, 0.025),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.02, 0.06),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 14),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 0.9),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 50),\n",
        "    }\n",
        "\n",
        "    # Create an instance of LGBMClassifier with the suggested parameters\n",
        "    lgbm_classifier = LGBMClassifier(**param)\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    lgbm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the classifier on the test data\n",
        "    score = lgbm_classifier.score(X_test, y_test)\n",
        "\n",
        "    print(f'SCORE: {score}')\n",
        "\n",
        "    return score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, label_encoded_targets, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n",
        "\n",
        "# Set up the sampler for Optuna optimization\n",
        "sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n",
        "\n",
        "# Create a study object for Optuna optimization\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "\n",
        "# If TUNE\n",
        "if TUNE:\n",
        "    # Run the optimization process\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=150)\n",
        "\n",
        "    # Get the best parameters after optimization\n",
        "    best_params = study.best_params"
      ],
      "metadata": {
        "id": "Xb4L6vsvnblb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial, X_train, y_train, X_test, y_test):\n",
        "    # Define parameters to be optimized for the XGBClassifier\n",
        "    param = {\n",
        "        \"objective\": 'multi:softmax',\n",
        "        \"booster\": 'gbtree',\n",
        "        \"random_state\": 42,\n",
        "        \"num_class\": 7,\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
        "        'gamma' : trial.suggest_float('gamma', 1e-9, 1.0),\n",
        "        'subsample': trial.suggest_float('subsample', 0.25, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.25, 1.0),\n",
        "        'max_depth': trial.suggest_int('max_depth', 0, 24),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 30),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),\n",
        "    }\n",
        "\n",
        "    # Create an instance of LGBMClassifier with the suggested parameters\n",
        "    xgb_classifier = XGBClassifier(**param)\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the classifier on the test data\n",
        "    score = xgb_classifier.score(X_test, y_test)\n",
        "    y_pred_prob = xgb_classifier.predict_proba(X_test)\n",
        "    lgbm_log_loss = log_loss(y_test, y_pred_prob)\n",
        "    print(f'SCORE: {score}')\n",
        "    print(f'Log Loss: {lgbm_log_loss}')\n",
        "\n",
        "    return score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, label_encoded_targets, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n",
        "\n",
        "# Set up the sampler for Optuna optimization\n",
        "sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n",
        "\n",
        "# Create a study object for Optuna optimization\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "\n",
        "if TUNE:\n",
        "\n",
        "    # Run the optimization process\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=150)\n",
        "\n",
        "    # Get the best parameters after optimization\n",
        "    best_params = study.best_params\n"
      ],
      "metadata": {
        "id": "lhuo5uIvnfDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_params: dict = {\n",
        "    \"class_weight\": class_weights_param, # Balanced class weight\n",
        "    \"objective\": \"multiclass\",          # Objective function for the model\n",
        "    \"metric\": \"multi_logloss\",          # Evaluation metric\n",
        "    \"verbosity\": -1,                    # Verbosity level (-1 for silent)\n",
        "    \"boosting_type\": \"gbdt\",            # Gradient boosting type\n",
        "    \"random_state\": 42,       # Random state for reproducibility\n",
        "    \"num_class\": 7,                     # Number of classes in the dataset\n",
        "    'n_estimators': 752,\n",
        "    'learning_rate': 0.005613916463106189,\n",
        "    'max_depth': 6,\n",
        "    'num_leaves': 252,\n",
        "    'subsample': 0.6045538705062335,\n",
        "    'colsample_bytree': 0.866501494211133,\n",
        "    'colsample_bynode': 0.5443098861233086,\n",
        "    'reg_alpha': 0.0024787490924597882,\n",
        "    'reg_lambda': 3.5334079815178954,\n",
        "    'min_split_gain': 0.05539710161546875,\n",
        "}"
      ],
      "metadata": {
        "id": "UxGVcZUpAEWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_params: dict = {\n",
        "    'class_weight': class_weights_param,\n",
        "    'objective':'multi:softmax',\n",
        "    'n_estimators': 829,\n",
        "    'learning_rate': 0.010260565670497695,\n",
        "    'gamma': 0.16282691057583543,\n",
        "    'reg_alpha': 0.010492176264956674,\n",
        "    'reg_lambda': 0.437536781187624,\n",
        "    'max_depth': 5,\n",
        "    'min_child_weight': 2,\n",
        "    'subsample': 0.6971737476610285,\n",
        "    'colsample_bytree': 0.5115061295805807,\n",
        "    'random_state': 345,\n",
        "}"
      ],
      "metadata": {
        "id": "AufUaJuJAHZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params: dict = {\n",
        "    'class_weights': class_weights_param,\n",
        "    'learning_rate': 0.13762007048684638,\n",
        "    'depth': 5,\n",
        "    'l2_leaf_reg': 5.285199432056192,\n",
        "    'bagging_temperature': 0.6029582154263095,\n",
        "    'random_seed': 42,\n",
        "    'verbose': False,\n",
        "    'iterations':1000,\n",
        "}"
      ],
      "metadata": {
        "id": "aRRSk6ghFw_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = [\n",
        "    ('XGB', XGBClassifier(**xgb_params)),\n",
        "    ('LGBM', LGBMClassifier(**lgbm_params)),\n",
        "    ('CAT', CatBoostClassifier(**cat_params))\n",
        "]"
      ],
      "metadata": {
        "id": "IySFUaXPFxNo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}