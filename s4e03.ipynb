{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjnFaFq3Kbo5"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "\n",
        "# Target encoding/decoding\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, auc, roc_curve, log_loss\n",
        "\n",
        "# Models\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier, plot_importance\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Math and DataFrame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Warnings ignore\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "original = pd.read_csv('/content/steel_plates_faults_original_dataset.csv')\n",
        "train.head()"
      ],
      "metadata": {
        "id": "gL7LQqc2Xz1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "FyfQd8VWX8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train, original], axis=0).drop_duplicates()\n",
        "train.head()"
      ],
      "metadata": {
        "id": "BwMDLxsIXLjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "LcS0cjg7XOXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop('id', axis=1)\n",
        "test = test.drop('id', axis=1)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "VwyyqZwsXOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_variables = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps',  'Other_Faults']"
      ],
      "metadata": {
        "id": "3Rb4-mcSXP2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the sum of the axis is greater than 1 because in the same row more than 1 columns value is 1, we can sure that is not a multi class problem, it is multi label problem.\n",
        "no_faults = train[train[target_variables].sum(axis=1) == 0][target_variables]\n",
        "print(f'\\nNumber of No Faults: {no_faults.shape[0]}\\n')\n",
        "no_faults"
      ],
      "metadata": {
        "id": "y0lGGGKRXTgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_faults.shape[0] / train.shape[0] * 100"
      ],
      "metadata": {
        "id": "IQr2nFXhXVYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "more_than_one_faults = train[train[target_variables].sum(axis=1) > 1][target_variables]\n",
        "print(f'\\nNumber of More than one faults: {more_than_one_faults.shape[0]}\\n')\n",
        "more_than_one_faults"
      ],
      "metadata": {
        "id": "B7vUQIT-tTsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train[target_variables].sum(axis=1) <= 1]\n",
        "train"
      ],
      "metadata": {
        "id": "4eLfdm5NtWn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a numpy array from our given targets.\n",
        "targets_arr = train[target_variables].values.copy()\n",
        "targets_arr"
      ],
      "metadata": {
        "id": "hDQHk5VltYfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_faults_arr = 1 - targets_arr.sum(axis=1)[:, np.newaxis]\n",
        "print(f'\\nNumber of the No Faults: {no_faults_arr.sum()}\\n')\n",
        "no_faults_arr"
      ],
      "metadata": {
        "id": "ahqIAILWtZd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_concatenated = np.concatenate([targets_arr, no_faults_arr], axis=1)\n",
        "targets_concatenated"
      ],
      "metadata": {
        "id": "SXfbgPDLtmaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(target_variables, axis=1)\n",
        "target = train[target_variables]\n",
        "X.head()"
      ],
      "metadata": {
        "id": "pE3Ukwdpto3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_sc = scaler.fit_transform(X)\n",
        "test_sc = scaler.transform(test)\n",
        "X_sc"
      ],
      "metadata": {
        "id": "e8csOSIYtqF8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}