{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "x2A2Bo4rBcL7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# First approach"
      ],
      "metadata": {
        "id": "x2A2Bo4rBcL7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjnFaFq3Kbo5"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "\n",
        "# Target encoding/decoding\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, auc, roc_curve, log_loss\n",
        "\n",
        "# Models\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier, plot_importance\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Math and DataFrame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Warnings ignore\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "test = pd.read_csv('/content/test.csv')\n",
        "original = pd.read_csv('/content/steel_plates_faults_original_dataset.csv')\n",
        "train.head()"
      ],
      "metadata": {
        "id": "gL7LQqc2Xz1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "FyfQd8VWX8vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train, original], axis=0).drop_duplicates()\n",
        "train.head()"
      ],
      "metadata": {
        "id": "BwMDLxsIXLjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "LcS0cjg7XOXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop('id', axis=1)\n",
        "test = test.drop('id', axis=1)\n",
        "train.head()"
      ],
      "metadata": {
        "id": "VwyyqZwsXOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_variables = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps',  'Other_Faults']"
      ],
      "metadata": {
        "id": "3Rb4-mcSXP2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the sum of the axis is greater than 1 because in the same row more than 1 columns value is 1, we can sure that is not a multi class problem, it is multi label problem.\n",
        "no_faults = train[train[target_variables].sum(axis=1) == 0][target_variables]\n",
        "print(f'\\nNumber of No Faults: {no_faults.shape[0]}\\n')\n",
        "no_faults"
      ],
      "metadata": {
        "id": "y0lGGGKRXTgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_faults.shape[0] / train.shape[0] * 100"
      ],
      "metadata": {
        "id": "IQr2nFXhXVYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "more_than_one_faults = train[train[target_variables].sum(axis=1) > 1][target_variables]\n",
        "print(f'\\nNumber of More than one faults: {more_than_one_faults.shape[0]}\\n')\n",
        "more_than_one_faults"
      ],
      "metadata": {
        "id": "B7vUQIT-tTsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train[train[target_variables].sum(axis=1) <= 1]\n",
        "train"
      ],
      "metadata": {
        "id": "4eLfdm5NtWn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a numpy array from our given targets.\n",
        "targets_arr = train[target_variables].values.copy()\n",
        "targets_arr"
      ],
      "metadata": {
        "id": "hDQHk5VltYfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_faults_arr = 1 - targets_arr.sum(axis=1)[:, np.newaxis]\n",
        "print(f'\\nNumber of the No Faults: {no_faults_arr.sum()}\\n')\n",
        "no_faults_arr"
      ],
      "metadata": {
        "id": "ahqIAILWtZd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_concatenated = np.concatenate([targets_arr, no_faults_arr], axis=1)\n",
        "targets_concatenated"
      ],
      "metadata": {
        "id": "SXfbgPDLtmaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop(target_variables, axis=1)\n",
        "target = train[target_variables]\n",
        "X.head()"
      ],
      "metadata": {
        "id": "pE3Ukwdpto3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_sc = scaler.fit_transform(X)\n",
        "test_sc = scaler.transform(test)\n",
        "X_sc"
      ],
      "metadata": {
        "id": "e8csOSIYtqF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot encoded to label-encoded"
      ],
      "metadata": {
        "id": "vN5iqiPqfEDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoded_targets = np.argmax(targets_concatenated, axis=-1)\n",
        "label_encoded_targets"
      ],
      "metadata": {
        "id": "oCzaBI4ntsXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_classes = np.unique(label_encoded_targets)\n",
        "unique_classes"
      ],
      "metadata": {
        "id": "-7HLhS2PfDh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=unique_classes, y=label_encoded_targets)\n",
        "class_weights"
      ],
      "metadata": {
        "id": "kXq-CwLSnFkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights_param = {key: value for key, value in zip(unique_classes, class_weights)}\n",
        "class_weights_param"
      ],
      "metadata": {
        "id": "uK5lr3IUnL6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna tuned parameters"
      ],
      "metadata": {
        "id": "uhOauZ3OnO1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TUNE = False"
      ],
      "metadata": {
        "id": "CVqMgZ7RnNM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial, X_train, y_train, X_test, y_test):\n",
        "    # Define parameters to be optimized for the LGBMClassifier\n",
        "    param = {\n",
        "        \"class_weight\": class_weights_param,\n",
        "        \"objective\": \"multiclass\",\n",
        "        \"metric\": \"multi_logloss\",\n",
        "        \"verbosity\": -1,\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"random_state\": 42,\n",
        "        \"num_class\": 7,\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.03),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 600),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.005, 0.025),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.02, 0.06),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 14),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 0.9),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 50),\n",
        "    }\n",
        "\n",
        "    # Create an instance of LGBMClassifier with the suggested parameters\n",
        "    lgbm_classifier = LGBMClassifier(**param)\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    lgbm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the classifier on the test data\n",
        "    score = lgbm_classifier.score(X_test, y_test)\n",
        "\n",
        "    print(f'SCORE: {score}')\n",
        "\n",
        "    return score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, label_encoded_targets, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n",
        "\n",
        "# Set up the sampler for Optuna optimization\n",
        "sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n",
        "\n",
        "# Create a study object for Optuna optimization\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "\n",
        "# If TUNE\n",
        "if TUNE:\n",
        "    # Run the optimization process\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=150)\n",
        "\n",
        "    # Get the best parameters after optimization\n",
        "    best_params = study.best_params"
      ],
      "metadata": {
        "id": "Xb4L6vsvnblb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial, X_train, y_train, X_test, y_test):\n",
        "    # Define parameters to be optimized for the XGBClassifier\n",
        "    param = {\n",
        "        \"objective\": 'multi:softmax',\n",
        "        \"booster\": 'gbtree',\n",
        "        \"random_state\": 42,\n",
        "        \"num_class\": 7,\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
        "        'gamma' : trial.suggest_float('gamma', 1e-9, 1.0),\n",
        "        'subsample': trial.suggest_float('subsample', 0.25, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.25, 1.0),\n",
        "        'max_depth': trial.suggest_int('max_depth', 0, 24),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 30),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 10.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 10.0, log=True),\n",
        "    }\n",
        "\n",
        "    # Create an instance of LGBMClassifier with the suggested parameters\n",
        "    xgb_classifier = XGBClassifier(**param)\n",
        "\n",
        "    # Fit the classifier on the training data\n",
        "    xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the classifier on the test data\n",
        "    score = xgb_classifier.score(X_test, y_test)\n",
        "    y_pred_prob = xgb_classifier.predict_proba(X_test)\n",
        "    lgbm_log_loss = log_loss(y_test, y_pred_prob)\n",
        "    print(f'SCORE: {score}')\n",
        "    print(f'Log Loss: {lgbm_log_loss}')\n",
        "\n",
        "    return score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, label_encoded_targets, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n",
        "\n",
        "# Set up the sampler for Optuna optimization\n",
        "sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n",
        "\n",
        "# Create a study object for Optuna optimization\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "\n",
        "if TUNE:\n",
        "\n",
        "    # Run the optimization process\n",
        "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=150)\n",
        "\n",
        "    # Get the best parameters after optimization\n",
        "    best_params = study.best_params\n"
      ],
      "metadata": {
        "id": "lhuo5uIvnfDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_params: dict = {\n",
        "    \"class_weight\": class_weights_param, # Balanced class weight\n",
        "    \"objective\": \"multiclass\",          # Objective function for the model\n",
        "    \"metric\": \"multi_logloss\",          # Evaluation metric\n",
        "    \"verbosity\": -1,                    # Verbosity level (-1 for silent)\n",
        "    \"boosting_type\": \"gbdt\",            # Gradient boosting type\n",
        "    \"random_state\": 42,       # Random state for reproducibility\n",
        "    \"num_class\": 7,                     # Number of classes in the dataset\n",
        "    'n_estimators': 752,\n",
        "    'learning_rate': 0.005613916463106189,\n",
        "    'max_depth': 6,\n",
        "    'num_leaves': 252,\n",
        "    'subsample': 0.6045538705062335,\n",
        "    'colsample_bytree': 0.866501494211133,\n",
        "    'colsample_bynode': 0.5443098861233086,\n",
        "    'reg_alpha': 0.0024787490924597882,\n",
        "    'reg_lambda': 3.5334079815178954,\n",
        "    'min_split_gain': 0.05539710161546875,\n",
        "}"
      ],
      "metadata": {
        "id": "UxGVcZUpAEWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_params: dict = {\n",
        "    'class_weight': class_weights_param,\n",
        "    'objective':'multi:softmax',\n",
        "    'n_estimators': 829,\n",
        "    'learning_rate': 0.010260565670497695,\n",
        "    'gamma': 0.16282691057583543,\n",
        "    'reg_alpha': 0.010492176264956674,\n",
        "    'reg_lambda': 0.437536781187624,\n",
        "    'max_depth': 5,\n",
        "    'min_child_weight': 2,\n",
        "    'subsample': 0.6971737476610285,\n",
        "    'colsample_bytree': 0.5115061295805807,\n",
        "    'random_state': 345,\n",
        "}"
      ],
      "metadata": {
        "id": "AufUaJuJAHZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params: dict = {\n",
        "    'class_weights': class_weights_param,\n",
        "    'learning_rate': 0.13762007048684638,\n",
        "    'depth': 5,\n",
        "    'l2_leaf_reg': 5.285199432056192,\n",
        "    'bagging_temperature': 0.6029582154263095,\n",
        "    'random_seed': 42,\n",
        "    'verbose': False,\n",
        "    'iterations':1000,\n",
        "}"
      ],
      "metadata": {
        "id": "aRRSk6ghFw_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = [\n",
        "    ('XGB', XGBClassifier(**xgb_params)),\n",
        "    ('LGBM', LGBMClassifier(**lgbm_params)),\n",
        "    ('CAT', CatBoostClassifier(**cat_params))\n",
        "]"
      ],
      "metadata": {
        "id": "IySFUaXPFxNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEIGHT_TUNE = False"
      ],
      "metadata": {
        "id": "rerzwzFpFzc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the objective function for Optuna optimization\n",
        "def objective(trial, X_train, y_train, X_test, y_test):\n",
        "    # Define parameters to be optimized for the weighted ensemble\n",
        "\n",
        "    obj_estimators = [\n",
        "        ('XGB', XGBClassifier(**xgb_params)),\n",
        "        ('LGBM', LGBMClassifier(**lgbm_params)),\n",
        "        ('CAT', CatBoostClassifier(**cat_params))\n",
        "    ]\n",
        "\n",
        "    voting_classifier = VotingClassifier(\n",
        "        estimators=obj_estimators,\n",
        "        voting='soft',\n",
        "        weights=[\n",
        "            trial.suggest_float('XGB_Weight', 1.00, 9.00),\n",
        "            trial.suggest_float('LGBM_Weight', 0.25, 5.00),\n",
        "            trial.suggest_float('CAT_Weight', 0.25, 2.00),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "    predict_probs = voting_classifier.predict_proba(X_test)\n",
        "\n",
        "    auc_score = roc_auc_score(y_test, predict_probs, multi_class='ovr')\n",
        "\n",
        "    return auc_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, label_encoded_targets, test_size=0.2, random_state=42)  # Adjust the test_size as needed\n",
        "\n",
        "# Set up the sampler for Optuna optimization\n",
        "weight_sampler = optuna.samplers.TPESampler(seed=42)  # Using Tree-structured Parzen Estimator sampler for optimization\n",
        "\n",
        "# Create a study object for Optuna optimization\n",
        "weight_study = optuna.create_study(direction=\"maximize\", sampler=weight_sampler)\n",
        "\n",
        "if WEIGHT_TUNE:\n",
        "\n",
        "    # Run the optimization process\n",
        "    weight_study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=300)\n",
        "\n",
        "    # Get the best parameters after optimization\n",
        "    weight_best_params = weight_study.best_params"
      ],
      "metadata": {
        "id": "3AfFT-4S9nZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting_estimators = [\n",
        "    ('XGB', XGBClassifier(**xgb_params)),\n",
        "    ('LGBM', LGBMClassifier(**lgbm_params)),\n",
        "    ('CAT', CatBoostClassifier(**cat_params))\n",
        "]"
      ],
      "metadata": {
        "id": "4yFdbqjz9nbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voting_classifier = VotingClassifier(\n",
        "    estimators=voting_estimators,\n",
        "    voting='soft',\n",
        "    weights=[\n",
        "            7.994761950304625,\n",
        "            0.46883567511191715,\n",
        "            0.4412142916220983\n",
        "        ],\n",
        "    )\n",
        "\n",
        "voting_classifier.fit(X_sc, label_encoded_targets)\n",
        "\n",
        "predicted_probs = voting_classifier.predict_proba(test_sc)[:, :-1] # Filtering out 8th's target (No Faults)"
      ],
      "metadata": {
        "id": "ahFWZZgIwA87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_probs.shape"
      ],
      "metadata": {
        "id": "s_fctfT9wCa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.read_csv('/kaggle/input/playground-series-s4e3/sample_submission.csv')\n",
        "submission_df\n",
        "\n",
        "submission_df[target_variables] = predicted_probs\n",
        "submission_df"
      ],
      "metadata": {
        "id": "21_IEhTVwHZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "HAMOQLv2wKan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second approach"
      ],
      "metadata": {
        "id": "JeAfNwJFBgGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import missingno as msno\n",
        "from prettytable import PrettyTable\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', font_scale=1.4)\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "tqdm_notebook.get_lock().locks = []\n",
        "# !pip install sweetviz\n",
        "# import sweetviz as sv\n",
        "import concurrent.futures\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from itertools import combinations\n",
        "import random\n",
        "from random import randint, uniform\n",
        "import gc\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler,PowerTransformer, FunctionTransformer\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from itertools import combinations\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xg\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.metrics import mean_squared_error,mean_squared_log_error, roc_auc_score, accuracy_score, f1_score, precision_recall_curve, log_loss\n",
        "from sklearn.cluster import KMeans\n",
        "!pip install yellowbrick\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "!pip install gap-stat\n",
        "from gap_statistic.optimalK import OptimalK\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import ttest_ind\n",
        "from scipy.stats import boxcox\n",
        "import math\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "!pip install optuna\n",
        "import optuna\n",
        "!pip install cmaes\n",
        "import cmaes\n",
        "import xgboost as xgb\n",
        "!pip install catboost\n",
        "!pip install lightgbm --install-option=--gpu --install-option=\"--boost-root=C:/local/boost_1_69_0\" --install-option=\"--boost-librarydir=C:/local/boost_1_69_0/lib64-msvc-14.1\"\n",
        "import lightgbm as lgb\n",
        "!pip install category_encoders\n",
        "from category_encoders import OneHotEncoder, OrdinalEncoder, CountEncoder, CatBoostEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, GradientBoostingClassifier,ExtraTreesClassifier, AdaBoostClassifier\n",
        "!pip install -U imbalanced-learn\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn.svm import NuSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from catboost import Pool\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.pandas.set_option('display.max_columns',None)"
      ],
      "metadata": {
        "id": "LOwQFGc4BhY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=pd.read_csv('/kaggle/input/playground-series-s4e3/train.csv')\n",
        "test=pd.read_csv('/kaggle/input/playground-series-s4e3/test.csv')\n",
        "original=pd.read_csv(\"/kaggle/input/steel-plates-faults/SteelPlatesFaults.csv\")\n",
        "\n",
        "train.drop(columns=[\"id\"],inplace=True)\n",
        "test.drop(columns=[\"id\"],inplace=True)\n",
        "\n",
        "train_copy=train.copy()\n",
        "test_copy=test.copy()\n",
        "original_copy=original.copy()\n",
        "\n",
        "print(original.shape)\n",
        "\n",
        "device='cpu'\n",
        "\n",
        "train=pd.concat([train,original],axis=0)\n",
        "train.reset_index(inplace=True,drop=True)\n",
        "\n",
        "target=['Pastry','Z_Scratch','K_Scatch','Stains','Dirtiness','Bumps','Other_Faults']\n",
        "\n",
        "original.head()"
      ],
      "metadata": {
        "id": "Ss5VdEizfUlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory analysis"
      ],
      "metadata": {
        "id": "n5pcH92Ue9YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cont_cols = test.columns\n",
        "colors = ['blue', 'orange', 'green']\n",
        "\n",
        "num_plots = len(cont_cols)\n",
        "num_cols = 3\n",
        "num_rows = -(-num_plots // num_cols)\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(21, 5 * num_rows))  # Adjust the figure size as needed\n",
        "\n",
        "for i, feature in enumerate(cont_cols):\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "\n",
        "    ax = axes[row, col] if num_rows > 1 else axes[col]\n",
        "\n",
        "    sns.histplot(train_copy[feature], kde=True, color=colors[0], label='Train', alpha=0.5, bins=30, ax=ax)\n",
        "    sns.histplot(test_copy[feature], kde=True, color=colors[1], label='Test', alpha=0.5, bins=30, ax=ax)\n",
        "    sns.histplot(original[feature], kde=True, color=colors[2], label='Original', alpha=0.5, bins=30, ax=ax)\n",
        "\n",
        "    ax.set_title(f'Distribution of {feature}')\n",
        "    ax.set_xlabel(feature)\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.legend()\n",
        "\n",
        "if num_plots % num_cols != 0:\n",
        "    for j in range(num_plots % num_cols, num_cols):\n",
        "        axes[-1, j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r_TLs338e5E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering"
      ],
      "metadata": {
        "id": "Hcj5HYTifbVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OHE(train_df,test_df,cols,target):\n",
        "    '''\n",
        "    Function for one hot encoding, it first combined the data so that no category is missed and\n",
        "    the category with least frequency can be dropped because of redunancy\n",
        "    '''\n",
        "    combined = pd.concat([train_df, test_df], axis=0)\n",
        "    for col in cols:\n",
        "        one_hot = pd.get_dummies(combined[col]).astype(int)\n",
        "        counts = combined[col].value_counts()\n",
        "        min_count_category = counts.idxmin()\n",
        "        one_hot = one_hot.drop(min_count_category, axis=1)\n",
        "        one_hot.columns=[str(f)+col for f in one_hot.columns]\n",
        "        combined = pd.concat([combined, one_hot], axis=\"columns\")\n",
        "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
        "\n",
        "    # split back to train and test dataframes\n",
        "    train_ohe = combined[:len(train_df)]\n",
        "    test_ohe = combined[len(train_df):]\n",
        "    test_ohe.reset_index(inplace=True,drop=True)\n",
        "    test_ohe.drop(columns=[target],inplace=True)\n",
        "    return train_ohe, test_ohe"
      ],
      "metadata": {
        "id": "wik1uLvQfC7d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}